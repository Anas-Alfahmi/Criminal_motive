{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Y7fLvA_rlI6",
        "outputId": "8d3c6b13-47b4-4061-e210-b2cc06818be6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11:09:13] WARNING: ../src/learner.cc:553: \n",
            "  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n",
            "  older XGBoost, please export the model by calling `Booster.save_model` from that version\n",
            "  first, then load it back in current version. See:\n",
            "\n",
            "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
            "\n",
            "  for more details about differences between saving model and serializing.\n",
            "\n",
            "Enter the username without '@': Anas_5x\n",
            "Profile image score: 1\n",
            "Profile image score: 1\n",
            "Tweets score: 5\n",
            "• No negative image has been detected.\n",
            "• No negative tweet has been detected.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from numpy.core.defchararray import join, mod\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "import os\n",
        "import torch\n",
        "from torch._C import device\n",
        "import torch.autograd.function as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from PIL import Image\n",
        "from torchvision import transforms as torchtrans  \n",
        "import re\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import pickle\n",
        "\n",
        "nltk.download('stopwords')\n",
        "ar_stopwords = set(nltk.corpus.stopwords.words('arabic'))\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "  # Preprocess the text data\n",
        "  # Normalize the text by removing diacritics and converting characters to their basic form\n",
        "  text = re.sub(r'[\\u064b-\\u065f\\u0640]','', text)\n",
        "  text = re.sub(r'[إأٱآا]', 'ا', text)\n",
        "  text = re.sub(r'[ئؤ]', 'ء', text)\n",
        "  # Remove non-Arabic characters and numbers\n",
        "  text = re.sub(r'[^\\u0600-\\u06FF\\s]','', text)\n",
        "  text = re.sub(r'\\d+','', text)\n",
        "  # Remove stop-words\n",
        "  tokens = [token for token in nltk.word_tokenize(text) if token not in ar_stopwords]\n",
        "  text = ' '.join(tokens)\n",
        "  return text\n",
        "\n",
        "def process_image(img):\n",
        "  img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "  img_res = img_rgb/255\n",
        "  img_res = torch.as_tensor(img_res).to(device)\n",
        "  img_res = img_res.permute(2, 0, 1)\n",
        "  return img_res=\n",
        "\n",
        "with open('model.pkl', 'rb') as f:\n",
        "    text_model = pickle.load(f)\n",
        "text_vectorizer = pickle.load(open('vectorizer.pkl', 'rb'))\n",
        "\n",
        "weapons_model_filename = \"weapon_trained_model-1.0a.pt\"\n",
        "\n",
        "cpu_device = torch.device(\"cpu\")\n",
        "weapons_model= torch.load(weapons_model_filename, map_location=torch.device('cpu'))\n",
        "weapons_model.eval()\n",
        "\n",
        "def predict_image(img):\n",
        "  input = []\n",
        "  input.append(img)\n",
        "  outputs = weapons_model(input)\n",
        "  outputs_list = outputs[0]['scores'].tolist()\n",
        "  if len(outputs_list) > 0: \n",
        "    return max(outputs_list) \n",
        "  else: \n",
        "    return 0\n",
        "\n",
        "def predict_text(text):\n",
        "  new_sentence = preprocess_text(text)\n",
        "  new_sentence_vectorized = text_vectorizer.transform([new_sentence])\n",
        "  predicted_label = text_model.predict(new_sentence_vectorized)[0]\n",
        "  return int(predicted_label)\n",
        "\n",
        "import tweepy\n",
        "import requests\n",
        "from textblob import TextBlob\n",
        "import cv2\n",
        "\n",
        "# Twitter API credentials\n",
        "consumer_key = \"***\"\n",
        "consumer_secret = \"***\"\n",
        "access_token = \"***\"\n",
        "access_token_secret = \"***\"\n",
        "\n",
        "# Connect to Twitter API\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "username = input(\"Enter the username without '@': \")\n",
        "\n",
        "user = api.get_user(screen_name=username)\n",
        "tweets = api.user_timeline(screen_name=username, count=5)\n",
        "favorites = api.get_favorites(screen_name=username, count=1)\n",
        "user_profile_image_url = user.profile_image_url_https\n",
        "response = requests.get(user_profile_image_url)\n",
        "with open(f\"{user.screen_name}_profile_image.jpg\", \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "profile_image = cv2.imread(f\"{user.screen_name}_profile_image.jpg\")\n",
        "profile_image_processed = process_image(profile_image)\n",
        "profile_image_score = 1 - predict_image(profile_image_processed)\n",
        "\n",
        "text_score = 0\n",
        "for tweet in tweets:    \n",
        "  text_score += predict_text(tweet.text)\n",
        "text_score = len(tweets) - text_score\n",
        "\n",
        "\n",
        "\n",
        "# print(\"Profile image score:\", profile_image_score)\n",
        "# print(\"Profile image score:\", int(profile_image_score))\n",
        "# print(\"Tweets score:\", text_score)\n",
        "\n",
        "\n",
        "\n",
        "if int(profile_image_score) <= 0:\n",
        "    print(\"• Warning: A negative image has been detected!\")\n",
        "    \n",
        "\n",
        "if text_score <= 4:\n",
        "    print(\"• Warning: A negative tweet has been detected!\")\n",
        "    \n",
        "if int(profile_image_score) <= 0 and text_score <= 4 :\n",
        "    print(\"• Warning: There is a possibility that the person may have criminal motives!\")\n",
        "    \n",
        "if int(profile_image_score) == 1:\n",
        "    print(\"• No negative image has been detected.\")\n",
        "    \n",
        "if text_score == 5:\n",
        "    print(\"• No negative tweet has been detected.\")\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nhl_TYIzsW9Z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}